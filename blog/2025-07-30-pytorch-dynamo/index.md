---
title: "Pytorch Dynamo"
date: 2025-07-30T15:00
---

Dynamo(æ¬¢è¿ä¸€é”®ä¸‰è¿ğŸ˜Š)

{/* truncate */}

## What is Dynamo

```python
@torch._dynamo.optimize(custom_compiler) # è£…é¥°å™¨ä½¿ç”¨ è‡ªå®šä¹‰backend
def fn(x, y):
    xxxxx
res = fn(torch.randn(10), torch.randn(10))

dynamo_callable = torch._dynamo.optimize(custom_compiler)(model/fn) # function API
```

```python
def fn(x, y):
    xxxx
new_fn = torch.compile(fn, backend="inductor") # compile å‡½æ•°å†…è°ƒç”¨optimize
```

â€œ PYTORCH 2.X: FASTER, MORE PYTHONIC AND AS DYNAMIC AS EVERâ€ã€‚

![å›¾ä¸€](./pytorch2.0.png "å›¾ä¸€")

â€œFASTERâ€ å’Œ â€œMore Pythonicâ€ -> torch2.0 å¼•å…¥äº† â€œ**a significant innovation that was a result of 5 years**â€ ï¼ˆå®˜æ–¹å‘è¨€ï¼‰æŠ€æœ¯ ***Dynamoã€‚é‚£ä»€ä¹ˆæ˜¯Dynamoå‘¢ï¼Ÿ***

     å®ƒæ˜¯ä¸€ä¸ªPython-level JIT compilerï¼šhook frame evalution API ï¼ˆCpython PEP 523ï¼‰ï¼ŒçœŸæ­£æ‰§è¡Œä¹‹å‰ä¿®æ”¹pythonå­—èŠ‚ç : åˆ†æpython operation trace fx graphï¼Œç„¶åäº¤ç»™ backend compiler è‡ªç”±å‘æŒ¥ã€‚pythonæ‰§è¡Œä¸backend compiler æ··åˆï¼Œæ—¢æœ‰å¯ç”¨æ€§åˆæœ‰ä¸é”™çš„æ€§èƒ½ ï¼ˆfaster and more pythonic å“ˆå“ˆå“ˆï¼‰ã€‚

***é‚£Dynamo ç›¸æ¯”äºä¹‹å‰torch æŠ“å›¾çš„æ–¹å¼ï¼Œæœ‰å•¥å¥½å¤„å‘¢ï¼š***

ç›¸æ¯”äºjit.trace/jit.script è½¬æ¨¡å‹ä»£ç éœ€è¦å¤§é‡ä¿®æ”¹æºç ï¼Œdynamoä¼šæ›´çµæ´»å’Œå®‰å…¨ã€‚dynamo break å›¾é€»è¾‘æ˜¯ä¾æ®å¼‚å¸¸æŠ¥é”™ï¼šå½“é‡åˆ°ä¸€ä¸ªä¸æ”¯æŒçš„opï¼Œæ¯”å¦‚ifï¼Œwhileï¼Œä¸ªåˆ«aten opç­‰ç­‰ä¼šè§¦å‘æ•å›¾ï¼Œä¸æ”¯æŒçš„aten opå›åˆ°eager æ‰§è¡Œã€‚ç”šè‡³è¯´é€šè¿‡å­—èŠ‚ç æˆ‘ä»¬å¯ä»¥æ•è·æ›´å¤šä¿¡æ¯ã€‚ä¸‹å›¾æ˜¯TrochDynamoæ•´ä½“ç»“æ„å›¾ï¼šé¦–å…ˆä¸€ä¸ªpython å‡½æ•°å¯¹åº”ä¸€ä¸ªPyFrameObjectï¼Œè¯¥FrameObjectä¿å­˜äº†è¯¥å‡½æ•°æ‰€éœ€è¦çš„å…¨å±€å˜é‡ã€å±€éƒ¨å˜é‡ã€å­—èŠ‚ç ç­‰ä¿¡æ¯ã€‚Dynamo æ˜¯å°†è¯¥å‡½æ•°å¯¹åº”çš„PyFrameObject æ”¹æ‰ï¼Œç„¶åå°†æ›´æ”¹åçš„PyFrameObjectå†è¿˜ç»™pythonè§£é‡Šå™¨æ‰§è¡Œã€‚

![å›¾äºŒ](./dynamo_arc.png "å›¾äºŒ")
## Why use Dynamo In XACC
XACCäº§å“å®šä½æ˜¯å¼€ç®±å³ç”¨çš„compilerï¼Œå¼€ç®±å³ç”¨çš„ç›®æ ‡æ˜¯å°½é‡ä¸æ”¹ç”¨æˆ·ä»£ç çš„åŒæ—¶åˆèƒ½æœ‰ä¸é”™æ€§èƒ½ï¼›compiler å˜›åˆ™æ˜¯ç”¨æ¥æé«˜æ€§èƒ½ã€‚æ€»ç»“æ¥è¯´XACCçš„ç»ˆæç›®æ ‡å°±æ˜¯æ—¢è¦æœ‰**æ³›åŒ–æ€§**åˆè¦æœ‰**æ€§èƒ½**ã€‚

ä¹‹å‰1.12 åŸºäºlazy tensorçš„å›¾æ¨¡å¼ä¼˜åŒ–å­˜åœ¨ä¸€äº›ç—›ç‚¹ï¼š

ç—›ç‚¹ä¸€ï¼šè™½ç„¶æ•´å›¾ä¼˜åŒ–æ”¶ç›Šæ›´é«˜ï¼Œä½†inplace, view/strided å¾ˆéš¾ä¼˜é›…å¤„ç†ã€‚

ç—›ç‚¹äºŒï¼šShape Inference è‡ªå®šä¹‰è¾ƒå¤š

ç—›ç‚¹ä¸‰ï¼šdynamic shape ä¸å¥½å¤„ç†

ç—›ç‚¹å››ï¼šè‡ªå·±ç»´æŠ¤ä¸€å¥—é€»è¾‘è¾ƒä¸ºå¤æ‚çš„å›¾cacheæœºåˆ¶ï¼Œå°è±¡ä¸­chenyongä¹‹å‰é‡åˆ°è¿‡â€œå›¾çš„è¾“å…¥å‚æ•°é¡ºåºæ¢äº†ï¼Œä½†ç¡®å‘½ä¸­äº†cacheæ± ä¸­å·²ç¼–è¯‘è¿‡çš„å›¾â€çš„é—®é¢˜ã€‚

æœ‰æ²¡æœ‰ä¸€ç§æ–¹å¼ï¼šæ—¢å¯ä»¥åˆ©ç”¨eager å¿«é€Ÿenable op kernel çš„æ³›åŒ–ç‰¹æ€§ï¼Œä¹Ÿå¯ä»¥åˆ©ç”¨å›¾æ¨¡å¼ä¼˜åŒ–æ€§èƒ½ï¼ŒåŒæ—¶è¿˜èƒ½ç¼“è§£é­”æ”¹ç”¨æˆ·ä»£ç çš„ç—›è‹¦ã€‚å½“ç„¶æ˜¯æœ‰çš„ï½torch2.0 çš„ä¸€ç³»åˆ—æ–°ç‰¹æ€§ï¼šdynamo, aot_autogard, prim ç­‰éƒ½æ˜¯åœ¨å¸®åŠ©backend compiler æé«˜å¼€å‘ï¼Œé›†æˆçš„ä½“éªŒï¼Œè®©æ›´å¤šçš„ç¬¬ä¸‰æ–¹å¼€å‘è€…åŠ å…¥åˆ°pytorchç”Ÿæ€ä¸­ã€‚

å†æ¬¡å›åˆ°Torch2.0 backend stack å›¾ï¼ˆå›¾ä¸€ï¼‰ã€‚Dynamo æœ¬è´¨å°±æ˜¯åˆ©ç”¨python VMæŠ“å›¾ï¼ŒæŠ“å‡ºæ¥çš„å›¾ç»è¿‡aot_autograd funcnalized åä¸¢ç»™backend compilerã€‚

åŸºäºlazy tensorçš„å›¾æ¨¡å¼ç—›ç‚¹ï¼Œtorchç‰ˆæœ¬å‡çº§ï¼Œæ ˆæ³›åŒ–æ€§èƒ½ç›®æ ‡ï¼Œå’ŒåŒäº‹ä»¬ç»è¿‡äº†å¾ˆé•¿æ—¶é—´çš„è®¨è®ºï¼Œç»å†äº†æœ€åˆçš„lazy+eageræ··åˆæ¨¡å¼æ–¹æ¡ˆï¼Œæœ€ç»ˆç¡®å®šåŸºäºdynamoå›¾ä¼˜åŒ–ä»¥æ­¤è¾¾åˆ°â€œ**æ³›åŒ–æ€§èƒ½â€**çš„ç›®æ ‡ã€‚

## æ­å¼€Dynamoâ€œç¥ç§˜â€é¢çº±
çŸ¥é“äº†dynamoçš„å‰ä¸–å’Œè¯ç”Ÿã€‚æ¥ä¸‹æ¥æˆ‘ä»¬ç»“åˆæºç æ¢å¯»Dyanmoï¼Œè¿™é‡Œä¸æ¶‰åŠè¯¦ç»†çš„python jitæœºåˆ¶ã€‚é™¤æ­¤ä¹‹å¤–è¿˜åŒ…æ‹¬fx graphï¼Œaot_auto_grad, ä»¥åŠå°‘éƒ¨åˆ†çš„inductor å›¾å¤„ç†æ–¹å¼ï¼ˆè¿™éƒ¨åˆ†æˆ‘ä¹‹åæ”¾åˆ°MLIR builderçš„è®¾è®¡æ–‡æ¡£ï¼Œè¿™é‡Œå†…å®¹å¤ªå¤šäº†22333ï¼‰ï¼Œå› ä¸ºä»¥ä¸Šè¿™äº›ç‰¹æ€§åœ¨XACC æ ˆæ¥FX Graph ï¼ˆXACC Builderï¼‰éƒ½ä¼šç”¨ä»¥åŠå€Ÿé‰´ğŸ˜Šã€‚

è¿™é‡Œå¯èƒ½éœ€è¦æå‰è¯´æ˜ï¼šè¿™ç¯‡æ–‡æ¡£ä¸»è¦èšç„¦åœ¨Dynamo æ˜¯å¦‚ä½•Trace Fx Graphã€‚å®é™…ä¸Šæˆ‘ä»¬çš„å·¥ä½œï¼ˆbackend compilerï¼‰æ˜¯åœ¨Dynamo æŠ“å®ŒFX Graphä¹‹åï¼šFX Graph -> MLIR IR -> IR pass -> è¿”å›ä¸€ä¸ªæ‰§è¡Œå‡½æ•°ã€‚

### æ›¿æ¢é»˜è®¤python è¯„ä¼°frame
å†å›é¡¾ä¸€éDynamoç»“æ„å›¾ï¼ˆéå¸¸é‡è¦ï¼Œå»ºè®®å¤šæ¬¡ğŸ‘€ï¼‰ï¼š

![dynamo](./dynamo_arc.png "")
è¿™å¼ å›¾çœŸçš„å¤ªç¥äº†ï¼Œå¸¸çœ‹å¸¸æœ‰æ–°çš„å‘ç°

![python_origin_call](./python_origin_call.png "")
Python VM é‡Œé¢å­˜æ”¾çš„éƒ½æ˜¯`PyCodeObject`ï¼ˆä¸‡ç‰©çš†å¯æ˜¯PyCodeObjectï¼‰ã€‚æ‰€æœ‰çš„å­—èŠ‚ç æŒ‡ä»¤å’Œç¨‹åºçš„é™æ€ä¿¡æ¯éƒ½å­˜æ”¾åœ¨`PyCodeObject`ä¸­ã€‚`PyFrameObjectï¼ˆå‡½æ•°æ‰§è¡Œæ—¶å¯¹åº”çš„æ ˆæ¡¢ï¼‰`åŒ…å«ï¼šä¿å­˜å‰ä¸€ä¸ªframeï¼Œæ ˆé¡¶åœ°å€ï¼Œæ ˆåº•åœ°å€ï¼Œå½“å‰æ‰§è¡Œçš„å­—èŠ‚ç ï¼ˆæŒ‡ä»¤ï¼‰ï¼Œä¸Šæ¡å­—èŠ‚ç ï¼Œå†…å­˜å¤§å°ï¼Œå±€éƒ¨å˜é‡ç­‰ç­‰ã€‚

å…³äº`PyCodeObject`å’Œ`PyFrameObject`çš„å…³ç³»ï¼Œæˆ‘çš„ç†è§£æ˜¯ï¼š

![dynamo_flow](./dynamo.drawio.png "")
è§£é‡Šå™¨å¯¹æ¯ä¸ªFrameï¼Œ éƒ½è¦è¯„ä¼°Frame (Frame Evaluation)ï¼Œè§£é‡Šå™¨åˆå§‹åŒ–æ—¶ä¼šå°†`tstate->interp->eval_frame`è®¾ç½®ä¸ºé»˜è®¤è¯„ä¼°frameå‡½æ•°ï¼Œå³PyEval_EvalFrameDefaultã€‚è§£é‡Šå™¨æ‰§è¡Œæ—¶ï¼Œæ‰§è¡Œ`MAKE_FUNCTION`å­—èŠ‚ç æ—¶ä¼šåˆ›å»º`PyFunctionObject`ï¼›æ‰§è¡Œ`CALL_FUNCTION`æ—¶åˆ›å»ºæ ˆæ¡¢ï¼Œç„¶åæ‰§è¡Œæ ˆæ¡¢ã€‚è°ƒç”¨æ ˆæµç¨‹å¦‚ä¸‹

![](./Interpreter.png "")
`CALL_FUNCTION`æ‰§è¡Œå‡½æ•°è¿‡ç¨‹å¦‚ä¸‹ï¼ˆæˆªå–äº†éƒ¨åˆ†ä»£ç ï¼‰ï¼š

```python
PyObject *
_PyEval_Vector(PyThreadState *tstate, PyFrameConstructor *con,
               PyObject *locals,
               PyObject* const* args, size_t argcount,
               PyObject *kwnames)
{
    // åˆ›å»ºæ ˆå¸§
    PyFrameObject *f = _PyEval_MakeFrameVector(
        tstate, con, locals, args, argcount, kwnames);
    // æ‰§è¡Œæ ˆå¸§
    PyObject *retval = _PyEval_EvalFrame(tstate, f, 0);
    return retval;
}


static inline PyObject*
_PyEval_EvalFrame(PyThreadState *tstate, PyFrameObject *f, int throwflag)
{
    return tstate->interp->eval_frame(tstate, f, throwflag); // eval_frame åœ¨åˆå§‹åŒ–çš„æ—¶å€™å°±è®¾ç½®æˆäº† '_PyEval_EvalFrameDefault'
}


PyObject* _Py_HOT_FUNCTION 
_PyEval_EvalFrameDefault(PyThreadState *tstate, PyFrameObject *f, int throwflag) // 
{
    // è¿æ¥åˆ°å‰ä¸€å¸§
    CFrame *prev_cframe = tstate->cframe;
    // åˆ‡æ¢å½“å‰å¸§
    tstate->frame = f;
    PyCodeObject *co = f->f_code;
    first_instr = (_Py_CODEUNIT *) PyBytes_AS_STRING(co->co_code);
    next_instr = first_instr + f->f_lasti + 1;
   
    for (;;) {
        _Py_CODEUNIT word = *next_instr;
        opcode = _Py_OPCODE(word);
        oparg = _Py_OPARG(word);
        next_instr++;
        switch (opcode) {
            // æ‰§è¡Œå­—èŠ‚ç 
        }
    }
}
```
**åŸºäºä¸Šè¿°ç®€å•çš„åŸºç¡€çŸ¥è¯†ï¼Œæˆ‘ä»¬å†æ¥çœ‹Dynamo : modelå†…éƒ¨æ‰€æœ‰çš„å‡½æ•°è°ƒç”¨ï¼Œåœ¨æ‰§è¡Œé»˜è®¤çš„eval framä¹‹å‰é™„åŠ ä¸€ä¸ªè§£æframeè¿‡ç¨‹ï¼Œä»¥æ­¤æŠ“å‡ºå›¾ç»“æ„ã€‚**

å’‹åšçš„å’§ï½ [PEP 523](https://peps.python.org/pep-0523/)ï¼Œæ›´æ”¹è§£é‡Šå™¨çš„`eval_frame`æ‰§è¡Œï¼Œå°†`eval_frame (_PyEval_EvalFrameDefault)`å‡½æ•°åœ°å€æ”¹ä¸ºDynamo é€šè¿‡ [set_eval_frame](https://link.zhihu.com/?target=https%3A//github.com/pytorch/pytorch/blob/e33f1eeeb73a8d680b8aae7944011389f76faaff/torch/csrc/dynamo/eval_frame.c%23L121)  è‡ªå·±å®ç°çš„custom frame evaluation å‡½æ•°ã€‚

![python_frame](./python_frame.png "")
### ByteCode ç¿»è¯‘ï¼ŒTrace Fx Graph
äº†è§£äº† Dynamo æ›¿æ¢`eval_frame`åŸç†ä¹‹åï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹å’‹åšçš„å­—èŠ‚ç ç¿»è¯‘ã€‚

`InstructionTranslator`ç±»è´Ÿè´£å­—èŠ‚ç çš„ç¿»è¯‘ï¼Œå®ƒåŒ…å«ä¸€ä¸ª`OutputGraph` (å…¶æ˜¯ä¸€ä¸ªfx.tracerï¼ˆä¿å­˜å­—èŠ‚ç ç¿»è¯‘åçš„è¾“å‡ºï¼Œä»¥åŠFxGraphï¼‰å®ä¾‹ï¼Œè¿˜åŒ…å«ä¸€ä¸ªå‰¯ä½œç”¨çš„è·Ÿè¸ªå™¨ã€‚`æˆ‘è‡ªå·±ç†è§£æ˜¯ `InstructionTranslator`æ›´åƒæ˜¯æ¨¡æ‹Ÿpython è™šæ‹Ÿæœºè¡Œä¸ºï¼Œå› ä¸ºè¦è¿‡ä¸€éå‡½æ•°ä¸­çš„æ‰€æœ‰å­—èŠ‚ç ã€‚`InstructionTranslator`è®°å½•äº†`instruction_pointer`ï¼ˆå½“å‰æ‰§è¡Œçš„å­—èŠ‚ç åœ¨pythonè™šæ‹Ÿæœºçš„ä½ç½®)ï¼Œæ•°æ®stackï¼Œblock_stack ç­‰ä¿¡æ¯ï¼Œé™¤æ­¤ä¹‹å¤–è¿˜å¯ä»¥æ‹¿åˆ° FrameObjectä¸­çš„çš„ä»»ä½•ä¿¡æ¯ï¼šå‡½æ•°ä»£ç ï¼Œå±€éƒ¨å˜é‡ï¼Œå…¨å±€å˜é‡ç­‰ï¼Œè¿™äº›ä¿¡æ¯æœ‰åˆ©äºæˆ‘ä»¬trace graphã€‚

#### åˆå§‹åŒ–`InstructionTranslator`
`InstructionTranslator`åˆå§‹åŒ–è¿‡ç¨‹ä¸­ä¼šå…ˆä¸ºè¾“å…¥å˜é‡åˆ›å»º`VariableTrack (Dynamo ç±»å‹ç³»ç»Ÿ)`: NNModuleVariable, ConstantVariableï¼Œ BuiltinVariableï¼ŒTorchVariableï¼Œ BaseUserFunctionVariable ç­‰ç­‰ã€‚åˆ›å»º`VariableTrack`çš„è¿‡ç¨‹ä¸­è¿˜ä¼šåˆ›å»º FXGraph Proxyï¼ŒFX Proxy æ›´åƒæ˜¯ä¸€ä¸ªGraph Nodeçš„æŠ½è±¡ï¼Œå®ƒè´Ÿè´£ä¸²è”å›¾ä¸­nodeå…³ç³»ï¼Œæ¯ä¸ªVariableTrack å’ŒProxy éƒ½æ˜¯ä¸€ä¸€å¯¹åº”çš„ã€‚

```python
 class VariableBuilder:
 def _wrap(self, value):
        from ..comptime import comptime
        make_guards = self.make_guards
        if istype(value, (torch.SymInt, torch.SymFloat)):
            return self.wrap_sym(value)
        if istensor(value): # é’ˆå¯¹å‡½æ•°è¾“å…¥çš„
            return self.wrap_tensor(value)
            
def wrap_tensor(self, value: torch.Tensor): 
    # åˆ›å»ºFakeTensorsï¼Œä¸å‘ç”ŸçœŸå®å†…å­˜åˆ†é…çš„Tensor
    tensor_variable = wrap_fx_proxy( # åˆ›å»ºFX Proxy å’Œ FX Nodeï¼Œé€šè¿‡è¯¥å‡½æ•°ï¼Œgraphçš„nodesä»¥åŠnodeçš„è¿æ¥å…³ç³»éƒ½æ­å»ºèµ·æ¥äº†ã€‚è¯¥å‡½æ•°åšäº†å¾ˆå¤šäº‹æƒ…ï½ä¹‹åå†æ¥è¯¦ç»†è¯´ä¸‹
                tx=self.tx,
                proxy=tensor_proxy,
                example_value=value,
                guards=self.make_guards(GuardBuilder.TENSOR_MATCH), # ä¸ºæ¯ä¸ªtensoråˆ›å»ºguard
                should_specialize=self.tensor_should_specialize(),
                ignore_subclass=ignore_subclass,
                source=self.get_source(),
            ) 
```
**åœ¨åˆ›å»º **`VariableTracker`**çš„è¿‡ç¨‹ä¸­ï¼ŒDynamo åœ¨ FX Graph ä¸­åˆ›å»ºäº† FX Proxyã€æ·»åŠ äº† Guardï¼ˆåç»­ä¼šå±•å¼€è¯´æ˜ä»€ä¹ˆæ˜¯Guardï¼‰ã€åˆ›å»ºäº†**`FakeTensor`**ï¼Œåˆå§‹åŒ–**`VariableTracker`ã€‚

#### Run `InstructionTranslator`ï¼šå­—èŠ‚ç ç¿»è¯‘
```python
 class InstructionTranslatorBase(Checkpointable[InstructionTranslatorGraphState]):
 def run(self):
        try:
            self.output.push_tx(self)
            while (
                self.instruction_pointer is not None
                and not self.output.should_exit
                and self.step() # æ¯æ¬¡stepå¤„ç†ä¸€ä¸ªå­—èŠ‚ç æŒ‡ä»¤ï¼Œä»pythonè™šæ‹Ÿæœºä¸­è·å–å½“å‰å¤„ç†å­—èŠ‚ç æŒ‡ä»¤çš„åœ°å€
            ):
                pass
```
é€æ¡ç¿»è¯‘å­—èŠ‚ç ï¼Œä¾æ¬¡åˆ›å»ºå‡½æ•°å†…éœ€è¦çš„æ‰€æœ‰å±€éƒ¨å˜é‡ã€‚***æ•æ‰åˆ°å¼‚å¸¸æˆ–è€… ***`RETURN_VALUE`*** æŒ‡ä»¤æ—¶è§¦å‘å›¾ç¼–è¯‘***ã€‚åœ¨æ•°æ®æ ˆä¸ºç©ºã€ä¸”åº”è¯¥ç¼–è¯‘å­å›¾çš„æ¡ä»¶ä¸‹ï¼Œ`InstructionTranslatorBase` ä¼šå¤‡ä»½å½“å‰çŠ¶æ€ä¸º checkpointï¼Œä»¥ä¾¿ä»¥åç”¨äºæ¢å¤ã€‚æ˜¯å¦è§¦å‘ç¼–è¯‘å­å›¾ï¼Œå–å†³äºå½“å‰ block stack ä¸­çš„æ‰€æœ‰æ¡ç›®æ˜¯å¦éƒ½å¯ä»¥æ¢å¤ã€å¹¶ä¸”ç”¨æˆ·æ²¡æœ‰é€šè¿‡ `one_graph` æˆ– `nopython` æŒ‡å®šå…¨å›¾ç¼–è¯‘ã€‚

```python
def step(self):
        """Process exactly one instruction, return False we should exit"""
        assert isinstance(self.instruction_pointer, int)
        inst = self.instructions[self.instruction_pointer] # instruction_pointer è·å–å½“å‰ step è¦å¤„ç†çš„å­—èŠ‚ç æŒ‡ä»¤
        self.current_instruction = inst
        self.instruction_pointer += 1

        log.debug(f"TRACE {inst.opname} {inst.argval} {self.stack}")

        try:
            if not hasattr(self, inst.opname):
                unimplemented(f"missing: {inst.opname}")
            getattr(self, inst.opname)(inst) # ç¿»è¯‘æ¯æ¡å­—èŠ‚ç 
            return inst.opname != "RETURN_VALUE"
        except Unsupported as exc:
            exc.real_stack.append(self.frame_summary())
            if self.empty_checkpoint():
                raise
            log.debug("step triggered compile", exc_info=True) # æ•è·å¼‚å¸¸ï¼Œè§¦å‘å­å›¾ç¼–è¯‘
            
        self.output.compile_subgraph( # åç«¯ç¼–è¯‘å­å›¾
            self,
            partial_convert=True,
            reason=GraphCompileReason("step_unsupported", [self.frame_summary()]),
        )
```
ä¸¾ä¾‹è¯´æ˜ï½

```python
@torch._dynamo.optimize(backend="inductor")
def fn(x, y):
    a = torch.cos(x)
    b = torch.sin(a)
    return a + b + y

103           0 LOAD_GLOBAL              0 (torch)
              2 LOAD_METHOD              1 (cos)
              4 LOAD_FAST                0 (x)
              6 CALL_METHOD              1
              8 STORE_FAST               2 (a)

104          10 LOAD_GLOBAL              0 (torch)
             12 LOAD_METHOD              2 (sin)
             14 LOAD_FAST                2 (a)
             16 CALL_METHOD              1
             18 STORE_FAST               3 (b)

105          20 LOAD_FAST                2 (a)
             22 LOAD_FAST                3 (b)
             24 BINARY_ADD
             26 LOAD_FAST                1 (y)
             28 BINARY_ADD
             30 RETURN_VALUE
```
é€æ¡è§£æå­—èŠ‚ç ï¼š

1. **Offset 0ï¼Œ** [LOAD_GLOBAL](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/symbolic_convert.py#L688): `inst.argval` ä¸º `torch`ï¼Œå®ƒä» `f_globals` ä¸­å–å‡ºåº“ `torch`ï¼Œè°ƒç”¨ `VariableBuilder(self, source)(value)` åœ¨ [builder.py#L391-L395](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/variables/builder.py#L391-L395) åˆ›å»ºäº† [TorchVariable](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/variables/torch.py#L123)ï¼Œå«ä¹‰æ˜¯ PyTorch ä¸­çš„æŸä¸ª packageï¼Œå¹¶åˆ›å»ºäº† `FUNCTION_MATCH` ç±»å‹çš„ `Guard`ã€‚æœ€åæŠŠè¯¥ `TorchVariable` å…¥æ ˆï¼Œæ­¤æ—¶æ ˆä¸Šçš„å†…å®¹ä¸º `[TorchVariable(torch)]`ã€‚

```python
def LOAD_GLOBAL(self, inst):
    # ...
    name = inst.argval
    # ...
    try:
        value = self.f_globals[name]
    except KeyError:
        return self.load_builtin(inst)

    source = self.get_global_source(name)
    self.push(VariableBuilder(self, source)(value))
```
2. **Offset 2**, [LOAD_METHOD](https://github.com/pytorch/pytorch/blob/fe05266fda4f908130dea7cbac37e9264c0429a2/torch/_dynamo/symbolic_convert.py#L1056),  [LOAD_ATTR](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/symbolic_convert.py#L1063): å…ˆå‡ºæ ˆä¸€ä¸ªå…ƒç´ ï¼Œå³ `TorchVariable(torch)`ï¼Œç„¶åä¸º `getattr` åˆ›å»ºäº† [BuiltinVariable](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/variables/builtin.py#L367)ã€ä¸º `inst.argval`ï¼ˆå³ `cos`ï¼‰ åˆ›å»ºäº† [ConstantVariable](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/variables/constant.py#L13)ï¼Œè½¬å…¥ [call_function()](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/variables/builtin.py#L428)ï¼Œ**å¯¹åº”ç€åœ¨ Dynamo ä¸­æ‰§è¡Œç±»ä¼¼ Python çš„ **`getattr(torch, cos)`** çš„åŠŸèƒ½ã€‚**è¿™é‡ŒVariableTrackæ‰§è¡Œpropagate()ï¼šæ”¶é›†è¾“å…¥ä¸­çš„æ‰€æœ‰ Guardï¼Œç„¶åå°è¯•å°†å‚æ•°ç»‘å®šåˆ°call_getatträ¸Šï¼Œç„¶åå†å»è°ƒç”¨call_getattr()ï¼Œcall_getattr() æ‰§è¡Œpropagate(): æ”¶é›†è¾“å…¥ä¸­çš„æ‰€æœ‰Guardï¼Œç„¶ååˆ›å»ºTorchVariable(torch.cos) ã€‚æ­¤æ—¶æ ˆä¸Šçš„å†…å®¹ä¸º `[TorchVariable(torch.cos)]`

```python
def LOAD_METHOD(self, inst):
        self.LOAD_ATTR(inst)
        self.push(self.pop())
        self.push(None)
        
 def LOAD_ATTR(self, inst):
        obj = self.pop()
        result = BuiltinVariable(getattr).call_function(
            self, [obj, ConstantVariable(inst.argval)], {}
        )
        self.push(result)
```
![](./dynamo_getattr.png "")

3. **Offset 4**, [LOAD_FAST](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/symbolic_convert.py#L633): `inst.argval` ä¸º xï¼Œä» `symbolic_locals(å±€éƒ¨å˜é‡)` å–å‡º å˜é‡ x çš„ `TensorVariable`ï¼Œç„¶åæŠŠå®ƒå‹åˆ°æ ˆä¸Šï¼Œæ ˆä¸Šçš„å†…å®¹ä¸º `TorchVariable(torch.cos),TensorVariable(x)]`ã€‚

```python
def LOAD_FAST(self, inst):
        name = inst.argval

        if name in self.f_locals and config.replay_record_enabled:
            self.exec_recorder.add_local_var(name, self.f_locals[name])

        if name.startswith(".") and name not in self.symbolic_locals:
            # This happens in dict/list comprehensions
            name = name.replace(".", "implicit")
        assert name not in self.cell_and_freevars()
        if name not in self.symbolic_locals:
            unimplemented("undefined LOAD_FAST")
        self.push(self.symbolic_locals[name])
        if name.startswith("___stack"):
            self.symbolic_locals.pop(name)
```
4. **Offset 6**,  `CALL_METHON`ä¸­å®é™…ä¸Šè°ƒç”¨çš„è¿˜æ˜¯[CALL_FUNCTION](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/symbolic_convert.py#L988): å› ä¸º `CALL_FUNCTION` è¢«è£…é¥°å™¨ [break_graph_if_unsupported()](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/symbolic_convert.py#L341) è£…é¥°ï¼Œæ‰€ä»¥æ‰§è¡Œ `CALL_FUNCTION()` ä¼šå…ˆç»è¿‡å…¶ä¸­çš„ `wrapper()`ã€‚è¿™é‡Œé¦–å…ˆåˆ›å»º checkpointï¼Œä¿å­˜äº†æ‰€æœ‰çš„çŠ¶æ€ï¼Œä»¥ä¾¿åœ¨åé¢å‡ºç°å¼‚å¸¸æ—¶ä» checkpoint ä¸­æ¢å¤ã€‚ ç„¶åæ‰§è¡Œ `inner_fn(self, inst)`ï¼Œ`inner_fn` å°±æ˜¯ `CALL_FUNCTION()`ï¼Œå…¶ä¸­å…ˆå‡ºæ ˆ N ä¸ªå…ƒç´ ä½œä¸ºå‡½æ•°å‚æ•°ï¼ŒN ç”± `inst.argval` æŒ‡å®šï¼Œè¿™é‡Œæ˜¯ 1ï¼Œç„¶åå†å‡ºæ ˆ 1 ä¸ªå…ƒç´ ä½œä¸ºå‡½æ•°å‚æ•°ï¼Œé€šè¿‡ [InstructionTranslatorBase.call_function()](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/symbolic_convert.py#L469) è¿›è¡Œå‡½æ•°è°ƒç”¨ã€‚



`InstructionTranslatorBase.call_function()` è°ƒç”¨ [TorchVariable.call_function()](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/variables/torch.py#L181)ï¼Œ**Dynamo åœ¨æ­¤å¤„æ¨¡æ‹Ÿæ‰§è¡Œ **`torch.cos(x)`ã€‚é¦–å…ˆç”¨ `propagate()` æ”¶é›†æ‰€æœ‰å‚æ•°ä¸­çš„ `Guard`ï¼Œç„¶ååŒ¹é…åˆ° [torch.py##L565-L573](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/variables/torch.py#L565-L573)ï¼Œæ­¤å¤„ [proxy_args_kwargs()](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/utils.py#L428) ä» `TensorVariable(a)` è·å– `torch.fx.Proxy(a)`ï¼Œå®ƒæ˜¯åœ¨åˆå§‹åŒ– `symbolic_locals` æ—¶åˆ›å»ºçš„ï¼Œç„¶åé€šè¿‡ [create_proxy()](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/output_graph.py#L783) åˆ›å»ºäº†æ–°çš„ `Proxy`ï¼Œç±»å‹æ˜¯ `call_function`ï¼Œç›®æ ‡æ˜¯ `torch.cos`ï¼Œå‚æ•°æ˜¯ xã€‚æœ€åé€šè¿‡ [wrap_fx_proxy](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/variables/builder.py#L864) åˆ›å»ºäº†æ–°çš„ `TensorVariable` æ¥ä¿å­˜ç»“æœï¼Œæ”¶é›†åˆ°çš„ `Guard` ä¿¡æ¯ä¹Ÿé™„åŠ äº†ä¸Šå»ï¼Œä¸€è·¯è¿”å›åå¹¶åœ¨ [call_function()](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/symbolic_convert.py#L469) å¤„å°†ç»“æœå‹æ ˆï¼Œæ ˆä¸Šçš„å†…å®¹ä¸º `[TensorVariable(torch.cos(x))]`

```python
def CALL_METHOD(self, inst):
        args = self.popn(inst.argval)
        dummy = self.pop()
        assert dummy is None
        fn = self.pop()
        self.call_function(fn, args, {})
     
@break_graph_if_unsupported(push=1)
def CALL_FUNCTION(self, inst):
    args = self.popn(inst.argval)
    fn = self.pop()
    self.call_function(fn, args, {})
```
5. **Offset 8**, [STORE_FAST](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/symbolic_convert.py#L659:STORE_FAST):  å‡ºæ ˆæ ˆé¡¶å…ƒç´ ï¼Œç„¶åå°†å…¶æ”¾å…¥åˆ°`symbolic_locals(å±€éƒ¨å˜é‡)`ã€‚æ ˆä¸Šçš„å†…å®¹ä¸º `[]`

```python
def STORE_FAST(self, inst):
        self.symbolic_locals[inst.argval] = self.pop()
```
6. ä¾æ¬¡ç±»æ¨ `LOAD_GLOBAL` `LOAD_METHOD` `LOAD_FAST` `CALL_METHOD` `LOAD_FAST LOAD_FAST`ã€‚æœ€åæ ˆä¸Šçš„å†…å®¹ä¸º `[TensorVariable(a),TensorVariable(b)]`
7. **Offset 24**, [BINARY_ADD](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/symbolic_convert.py#L1479): `InstructionTranslatorBase` å¯¹å¸¸è§çš„ Python å†…å»ºå‡½æ•°ç”¨ [stack_op](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/symbolic_convert.py#L144) åšäº†å°è£…å’Œè½¬å‘ï¼Œå…¶ä¸­ `BINARY_ADD = stack_op(operator.add)`ã€‚

```python
BINARY_ADD = stack_op(operator.add)

def stack_op(fn: typing.Callable[..., object]):
    nargs = len(inspect.signature(fn).parameters)
    fn_var = BuiltinVariable(fn)

    @functools.wraps(fn)
    def impl(self: "InstructionTranslatorBase", inst: Instruction):
        self.push(fn_var.call_function(self, self.popn(nargs), {}))

    return impl
```
å…¶ä¸­ï¼Œ`fn_var` æ˜¯åˆ›å»ºé—­åŒ…æ—¶åˆ›å»ºçš„ [BuiltinVariable](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/variables/builtin.py#L367)ï¼Œéœ€è¦å‡ºæ ˆçš„å‚æ•°ä¸ªæ•°ç”± `inspect.signature(fn)` ç¡®å®šï¼Œå¯¹äº `operator.add` æ¥è¯´éœ€è¦ 2 ä¸ªå‚æ•°ï¼Œå› æ­¤å‡ºæ ˆ `TensorVariable(a)` å’Œ `TensorVariable(b)`ï¼Œéšåè½¬åˆ° [BuiltinVariable.call_function()](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/variables/builtin.py#L428)ï¼Œå…¶ä¸­è°ƒç”¨ `propagate()` ä»è¾“å…¥çš„ `VariableTracker` ä¸­æ”¶é›† `Guard`ï¼Œåˆ†åˆ«æ˜¯é’ˆå¯¹å˜é‡ x çš„ `TENSOR_MATCH` å’Œé’ˆå¯¹ torch çš„ `FUNCTION_MATCH`ã€‚ æ­¤æ—¶æ ˆå†…å®¹`TensorVariable(a+b)`

8.  ä¾æ¬¡ç±»æ¨ `LOAD_FAST`å’Œ `BINARY_ADD`, æœ€åæ ˆçš„å†…å®¹ä¸º `TensorVariable(y+a+b)`
9.  **Offset 30**,  RETURN_VALUEï¼Œ è§¦å‘compile graphï¼ˆä¹‹åå†è¯´è¿™é‡Œï¼‰

Hi Hi Shape Infer âœ¨âœ¨âœ¨

æœ‰ä¸ªç‚¹å¾ˆå¥½ç©ï½æœ€æ—©çš„æ—¶å€™ä¸€ç›´æ²¡æœ‰æƒ³æ˜ç™½ â€œFX Graphçš„æ—¶å€™ä»€ä¹ˆæ—¶å€™åšçš„shape inferâ€ã€‚å…¶å®å°±æ˜¯åœ¨ `wrap_fx_proxy`ä¸­

é€šè¿‡ `example_value = get_fake_value(proxy.node, tx)` **ä» FX Node ä¸­åˆ›å»º **`FakeTensor`**ã€å¹¶ä»¥ **`FakeTensor`** è¿è¡Œè¯¥èŠ‚ç‚¹æ‰€ä»£è¡¨çš„ç®—å­**ï¼Œå®ç°åœ¨ [get_fake_value()](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/utils.py#L1120)ã€‚**æ ¹æ®Dispatcheræœºåˆ¶ï¼Œå®ƒå¯ä»¥åœ¨ Python å±‚é¢æ•è·åˆ° PyTorch æœ€åº•å±‚çš„ ATen ç®—å­æˆ–è€…æ˜¯primç®—å­ã€‚**

```
home/pytorch2.0/torch/_dynamo/variables/torch.py(548)call_function() # call_functionè°ƒç”¨ï¼Œç„¶ååœ¨è¯¥functioné‡Œä¼šé€æ­¥è°ƒç”¨åˆ°torch.opså‡½æ•°ã€‚
-> tensor_variable = wrap_fx_proxy( # wrap_fx_proxy 
  /home/pytorch2.0/torch/_dynamo/variables/builder.py(756)wrap_fx_proxy()
-> return wrap_fx_proxy_cls(
  /home/pytorch2.0/torch/_dynamo/variables/builder.py(791)wrap_fx_proxy_cls()
-> example_value = get_fake_value(proxy.node, tx). # è¿™é‡Œä»FX Nodeä¸­åˆ›å»ºFakeTensorï¼Œ
  /home/pytorch2.0/torch/_dynamo/utils.py(1152)get_fake_value()
-> return wrap_fake_exception(
  /home/pytorch2.0/torch/_dynamo/utils.py(808)wrap_fake_exception()
-> return fn()
  /home/pytorch2.0/torch/_dynamo/utils.py(1153)<lambda>()
-> lambda: run_node(tx.output, node, args, kwargs, nnmodule) # ä»¥FakeTensors è¿è¡Œç®—å­
  /home/pytorch2.0/torch/_dynamo/utils.py(1194)run_node() 
-> return node.target(*args, **kwargs)      # è¿™é‡Œå¼€å§‹torch.dispatch åˆ°opçš„å¯¹åº”function
  /home/pytorch2.0/torch/utils/_stats.py(20)wrapper()
-> return fn(*args, **kwargs)
  /home/pytorch2.0/torch/_subclasses/fake_tensor.py(987)__torch_dispatch__()
-> return self.dispatch(func, types, args, kwargs)
  /home/pytorch2.0/torch/_subclasses/fake_tensor.py(1170)dispatch()
-> r = func(*args, **kwargs)
  /home/pytorch2.0/torch/_ops.py(287)__call__()
-> return self._op(*args, **kwargs or {})
  /home/pytorch2.0/torch/_prims_common/wrappers.py(220)_fn()
-> result = fn(*args, **kwargs)
  /home/pytorch2.0/torch/_prims_common/wrappers.py(344)_fn()
-> return fn(*args, **kwargs)
  /home/pytorch2.0/torch/_prims_common/wrappers.py(130)_fn()
-> result = fn(**bound.arguments) # è¿™é‡Œçš„bound.arguments è¦ç¡®ä¿æ˜¯fake tensors
  /home/pytorch2.0/torch/_refs/__init__.py(402)_ref()
-> return prim(a)
  /home/pytorch2.0/torch/_refs/__init__.py(507)cos()
-> return prims.cos(a).   # è¿™é‡Œè°ƒåˆ°cos function
  /home/pytorch2.0/torch/_ops.py(287)__call__()
-> return self._op(*args, **kwargs or {})
  /home/pytorch2.0/torch/_prims/__init__.py(388)_elementwise_meta()
```
#### è§¦å‘ç¼–è¯‘subgraphçš„å‡ ç§åœºæ™¯ï¼š
1. return_value
2. generic_jumpï¼ˆif-else åˆ†æ”¯è¯­å¥ï¼‰
3. step (python å­—èŠ‚ç å½“ä¸­çš„step)
4. STORE_ATTR ï¼šå‰¯ä½œç”¨ï¼Œå…¨å±€çŠ¶æ€çš„æ”¹å˜ã€‚
5. break_graph_if_unsupported (è°ƒç”¨call functionå¦‚æœä¸æ”¯æŒï¼Œåˆ™å‡ºå‘trigger compile)

è§¦å‘å­å›¾ä¹‹åä¼šè¿›å…¥`compile_subgraph`

```python
self.restore_graphstate(state)
self.output.compile_subgraph(self, reason=reason)
self.popn(push - dis.stack_effect(inst.opcode, inst.arg))
```
1. åˆ†ælive variable, ä»`symbolic_locals`remove dead variable
2. æ¥ç€ä¸Šé¢ğŸŒ°ï¼šè¿›å…¥compile_subgraphåï¼Œæ ˆçš„å†…å®¹ä¸º`TensorVariable(y+a+b)`

```python
  def compile_subgraph(
        self, tx, partial_convert=False, reason: Optional[GraphCompileReason] = None
    ):
 # optimization to generate better code in a common case
            self.add_output_instructions(
                self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root) # stack_val: TensorVariable(y+a+b)
                + [create_instruction("UNPACK_SEQUENCE", len(stack_values))]
            )
```
[compile_and_call_fx_graph()](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/output_graph.py#L583) ç”¨äº **ä» **`OutputGraph`** ä¸­ç”Ÿæˆä»£ç **ã€‚å…¶ä¸­é¦–å…ˆæ ¹æ®ä¼ å…¥çš„ä¸¤ä¸ªè¾“å‡ºèŠ‚ç‚¹æ›´æ–°äº† Guardï¼Œ[create_node()](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/output_graph.py#L823) åœ¨ FX Graph ä¸­åˆ›å»ºäº†ç±»å‹ä¸º `output` çš„ FX Proxyï¼Œ**ä¸€å¼ å®Œæ•´çš„ FX Graph åˆ°æ­¤æ„å»ºå®Œæ¯•**ã€‚[remove_unused_graphargs()](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/output_graph.py#L724) ä» FX Graph ä¸­åˆ é™¤æ²¡æœ‰ `user` çš„èŠ‚ç‚¹ã€‚`fx.GraphModule(root, self.graph)` ä» `fx.Graph` åˆ›å»º `fx.GraphModule`ï¼Œé€šè¿‡ [recompile()](https://github.com/pytorch/pytorch/blob/fe05266/torch/fx/graph_module.py#L634) ç”Ÿæˆå¯¹åº”çš„ Python ä»£ç ï¼Œç”Ÿæˆçš„pythonä»£ç é€šè¿‡python å†…ç½®å‡½æ•°`exec`ç¼–è¯‘ä¸ºå­—èŠ‚ç ï¼ˆå¯æ‰§è¡Œå‡½æ•°ï¼‰ï¼Œæ–°ç¼–è¯‘çš„å‡½æ•°åä¸º `__compiled_fn_0()`ï¼Œä½œä¸ºGraphModuleçš„Forwardå‡½æ•°ï¼Œ

```python
    def compile_and_call_fx_graph(self, tx, rv, root):
        """
        Generate code from self.graph and return the Instruction()s to
        call that generated code.
        """
        from .eval_frame import disable
        
        for output in rv:
            self.guards.update(output.guards) # æ›´æ–°guards

        # åˆ›å»ºæœ€åä¸€ä¸ª output node: TensorVariable(y+a+b)
        self.create_node(
            "output", "output", (self.create_arg(tuple(x.as_proxy() for x in rv)),), {}
        )
        #åˆ é™¤æ²¡æœ‰ç”¨åˆ°çš„graph args
        self.remove_unused_graphargs()
        ncalls = count_calls(self.graph)
        counters["stats"]["calls_captured"] += ncalls
        counters["stats"]["fusions_possible"] += ncalls - 1

        # free a bit of memory
        for node in self.graph.nodes:
            if "example_value" in node.meta:
                del node.meta["example_value"]
        self.real_value_cache.clear()

        gm = fx.GraphModule(root, self.graph)  # å“ˆå“ˆå“ˆ è¿™ä¸ªåœ°æ–¹å°±åˆ›å»ºå‡ºæ¥äº†graphmodulerï½ï½
        gm.recompile()  # è¿™ä¸ªåœ°æ–¹è¦ç”Ÿæˆå¯¹åº”çš„python codeã€‚é€šè¿‡ç”Ÿæˆpython codeæ¥è·å¾—æ”¹å†™åçš„å­—èŠ‚ç 
        gm.compile_subgraph_reason = self.compile_subgraph_reason
        name = unique_id("__compiled_fn")

        assert_no_fake_params_or_buffers(gm)
        with tracing(self.tracing_context):
            compiled_fn = self.call_user_compiler(gm) # è°ƒç”¨backend compiler æä¾›çš„compile å‡½æ•°ç¼–è¯‘gm
        compiled_fn = disable(compiled_fn) # å·²ç»ç¼–è¯‘è¿‡çš„å‡½æ•°ä¸ä¼šå†ç¬¬äºŒæ¬¡ç¼–è¯‘
        counters["stats"]["unique_graphs"] += 1
        self.install_global(name, compiled_fn)
        
        cg = PyCodegen(tx) # ç”Ÿæˆå­—èŠ‚ç 
        cg.make_call_generated_code(name)
        return cg.get_instructions()
```

```python
#åŸå§‹å‡½æ•°ï¼š
def fn(x, y):
    a = torch.cos(x)
    b = torch.sin(a)
    return a + b + y

def forward(self, x : torch.Tensor, y : torch.Tensor):
    cos = torch.cos(x);  x = None
    sin = torch.sin(cos)
    add = cos + sin;  cos = sin = None
    add_1 = add + y;  add = y = None
    return (add_1,)
```

> æ’æ’­ï¼šå¦‚ä½•å°†FXGraph è½¬ä¸ºpythonä»£ç ï¼šé¦–å…ˆæŠŠä¸€äº›å†…ç½®åç§°æ·»åŠ åˆ°å…¨å±€å‘½åç©ºé—´ï¼Œä¾‹å¦‚ `inf, nan, None, torch `ã€‚ç„¶åä¾æ¬¡éå†å›¾ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ï¼Œæ‰¾åˆ°æ¯ä¸ªèŠ‚ç‚¹æœ€åè¢«ä½¿ç”¨çš„åœ°æ–¹ï¼Œä»è€Œåœ¨ä»£ç ç”Ÿæˆçš„è¿‡ç¨‹ä¸­åŠæ—¶é‡Šæ”¾ä¸ç”¨çš„èŠ‚ç‚¹ã€‚æ”¹å†™å®Œçš„pythonå­—èŠ‚ç ï¼š

```
0 LOAD_GLOBAL              3 (__compiled_fn_0)
2 LOAD_FAST                0 (x)
4 LOAD_FAST                1 (y)
6 CALL_FUNCTION            2
8 UNPACK_SEQUENCE          1
10 RETURN_VALUE
```
å¯¹åº”çš„FX Graph ä¸ºï¼š

```
__compiled_fn_0 <eval_with_key>.5 opcode         name    target                                                  args         kwargs
-------------  ------  ------------------------------------------------------  -----------  --------
placeholder    x       x                                                       ()           {}
placeholder    y       y                                                       ()           {}
call_function  cos     <built-in method cos of type object at 0x7fb5189f8800>  (x,)         {}
call_function  sin     <built-in method sin of type object at 0x7fb5189f8800>  (cos,)       {}
call_function  add     <built-in function add>                                 (cos, sin)   {}
call_function  add_1   <built-in function add>                                 (add, y)     {}
output         output  output                                                  ((add_1,),)  {}
```
è‡³æ­¤compile subgraph ç®—æ˜¯åŸºæœ¬å®Œæˆäº†ã€‚

### Multi-Subgraph
ä¸¾ä¸ªğŸŒ°ï¼š

```python
def toy_example(a, b):
    x = a / (torch.abs(a) + 1)
    if b.sum() < 0:
        b = b * -1
    return x * b
for _ in range(100):
    toy_example(torch.randn(10), torch.randn(10))s
```
```
224           0 LOAD_FAST                0 (a)
              2 LOAD_GLOBAL              0 (torch)
              4 LOAD_METHOD              1 (abs)
              6 LOAD_FAST                0 (a)
              8 CALL_METHOD              1
             10 LOAD_CONST               1 (1)
             12 BINARY_ADD
             14 BINARY_TRUE_DIVIDE
             16 STORE_FAST               2 (x)

225          18 LOAD_FAST                1 (b)
             20 LOAD_METHOD              2 (sum)
             22 CALL_METHOD              0
             24 LOAD_CONST               2 (0)
             26 COMPARE_OP               0 (<)
             28 POP_JUMP_IF_FALSE       38  # è§¦å‘å­å›¾ç¼–è¯‘

226          30 LOAD_FAST                1 (b)
             32 LOAD_CONST               3 (-1)
             34 BINARY_MULTIPLY
             36 STORE_FAST               1 (b)

227     >>   38 LOAD_FAST                2 (x)
             40 LOAD_FAST                1 (b)
             42 BINARY_MULTIPLY
             44 RETURN_VALUE
```
`POP_JUMP_IF_FALSE`è§¦å‘å­å›¾ç¼–è¯‘ï¼Œç¬¬ä¸€å¼ FX Graph å¦‚ä¸‹ï¼š

```
 __compiled_fn_0 <eval_with_key>.5 opcode         name     target                                                  args              kwargs
-------------  -------  ------------------------------------------------------  ----------------  --------
placeholder    a        a                                                       ()                {}
placeholder    b        b                                                       ()                {}
call_function  abs_1    <built-in method abs of type object at 0x7f4cbadc8800>  (a,)              {}
call_function  add      <built-in function add>                                 (abs_1, 1)        {}
call_function  truediv  <built-in function truediv>                             (a, add)          {}
call_method    sum_1    sum                                                     (b,)              {}
call_function  lt       <built-in function lt>                                  (sum_1, 0)        {}
output         output   output                                                  ((truediv, lt),)  {}
```

å¯¹åº”çš„python å‡½æ•°ä¸ºï¼š
```python
def forward(self, a : torch.Tensor, b : torch.Tensor):
    abs_1 = torch.abs(a)
    add = abs_1 + 1;  abs_1 = None
    truediv = a / add;  a = add = None
    sum_1 = b.sum();  b = None
    lt = sum_1 < 0;  sum_1 = None
    return (truediv, lt)
```
æ”¹å†™åçš„å­—èŠ‚ç å¦‚ä¸‹ï¼š

```
222           0 LOAD_GLOBAL              3 (__compiled_fn_0) # å¯¹åº”ä¸Šè¿°æµç¨‹ï¼Œå‰é¢ä¹Ÿä¸¾äº†ä¾‹å­è®²ï¼Œæœ‰å…´è¶£å¯ä»¥è‡ªå·±æ¨ä¸€ä¸‹ã€‚
              2 LOAD_FAST                0 (a) # compiled_fn_0
              4 LOAD_FAST                1 (b)
              6 CALL_FUNCTION            2
              8 UNPACK_SEQUENCE          2 # ç”¨äºæ‹†åˆ†ä½œä¸ºå…ƒç»„è¿”å›çš„å‡½æ•°ç»“æœ
              # æ ˆä¸Šè¿˜æœ‰TensorVariable x (live tensor) ä½œä¸º TensorVariableä¹‹åè¿˜ä¼šè¢«ç”¨åˆ°
             10 STORE_FAST               2 (x) 
             12 POP_JUMP_IF_FALSE       24
             14 LOAD_GLOBAL              4 (__resume_at_30_1) # if ä¸ºTrueçš„å­—èŠ‚ç æŒ‡ä»¤ 30_1 æ˜¯æŒ‡ä»¤çš„åç§»é‡
             16 LOAD_FAST                1 (b)
             18 LOAD_FAST                2 (x)
             20 CALL_FUNCTION            2
             22 RETURN_VALUE
        >>   24 LOAD_GLOBAL              5 (__resume_at_38_2) # if ä¸ºFalseçš„å­—èŠ‚ç æŒ‡ä»¤
             26 LOAD_FAST                1 (b)
             28 LOAD_FAST                2 (x)
             30 CALL_FUNCTION            2
             32 RETURN_VALUE
             
```

![](./dynamo_call_resume.png "")

```python
    def create_call_resume_at(self, inst):
        self.instruction_pointer = None

        if inst.opname == "RETURN_VALUE":
            return [create_instruction("RETURN_VALUE")]

        reads = livevars_analysis(self.instructions, inst) # åˆ†ææ´»è·ƒçš„å­—èŠ‚ç 
        argnames = tuple(
            k
            for k in self.symbolic_locals.keys()
            if k in reads and k not in self.cell_and_freevars()
        )
        nargs = len(self.stack) + len(argnames)

        name = unique_id(f"__resume_at_{inst.offset}")

        new_code: types.CodeType = ContinueExecutionCache.lookup( # æŸ¥ä¸åˆ°å°±ä¼šç”Ÿæˆæ–°çš„ä»£ç 
            self.f_code,
            self.lineno,
            inst.offset,
            len(self.stack),
            argnames,
            tuple(b.resume_fn() for b in self.block_stack),
        )
        
        # Notice: è¿™é‡Œç”Ÿæˆçš„python ä»£ç è¿˜æ²¡æœ‰è¿›è¡Œç¼–è¯‘
   
        cg = PyCodegen(self)

        if new_code.co_freevars:
            cg.make_function_with_closure(name, new_code, len(self.stack))
        else:
            self.output.install_global(
                name, types.FunctionType(new_code, self.f_globals, name)
            )
            cg.extend_output(cg.load_function_name(name, len(self.stack)))

        cg.extend_output([cg.create_load(k) for k in argnames])
        cg.extend_output(
            [
                create_instruction("CALL_FUNCTION", nargs),
                create_instruction("RETURN_VALUE"),
            ]
        )
         # 14 LOAD_GLOBAL              4 (__resume_at_30_1) # if ä¸ºTrueçš„å­—èŠ‚ç æŒ‡ä»¤ 30_1 æ˜¯æŒ‡ä»¤çš„åç§»é‡
         # 16 LOAD_FAST                1 (b)
         # 18 LOAD_FAST                2 (x)
         # 20 CALL_FUNCTION            2
         # 22 RETURN_VALUE
        return cg.get_instructions()
```
æ€»ç»“ä¸€ä¸‹ï¼šåŸå§‹å‡½æ•°çš„å­—èŠ‚ç ä¸­ç¬¬ä¸€æ¬¡dynamoç¼–è¯‘å®Œæˆåï¼Œæ–°çš„å­—èŠ‚ç ä¸­å¤šäº†ä¸‰æ¬¡`CALL_FUNCTION`ã€‚

### Eexcute æ‰§è¡Œ
å­å›¾ç¼–è¯‘å®Œä¹‹åä¼šè¿”å›åˆ° `_custom_eval_frame()è¯¥ä»£ç åœ¨evel_frame.c ä¸­`:

```c
static PyObject* _custom_eval_frame_shim(
    PyThreadState* tstate,
    THP_EVAL_API_FRAME_OBJECT* frame,
    int throw_flag) {
  // Shims logic into one of three states. Can probably be refactored into a
  // single func, later:
  //  - None: disables TorchDynamo
  //  - False: run-only mode (reuse existing compiles)
  //  - Python callable(): enables TorchDynamo
  PyObject* callback = eval_frame_callback_get();

  if (callback == Py_None) {
    return eval_frame_default(tstate, frame, throw_flag);
  }

  return _custom_eval_frame(tstate, frame, throw_flag, callback); # è¿™é‡Œè¿™é‡Œï½ï½
}
```

```c
PyObject* result =
    call_callback(callback, frame, cache_size(extra));
if (result == NULL) {
  // internal exception, returning here will leak the exception into user code
  // this is useful for debugging -- but we dont want it to happen outside of
  // testing
  return NULL;
} else if (result != Py_None) {
  DEBUG_TRACE("create cache %s", name(frame));
  extra = create_cache_entry(extra, result);
  Py_DECREF(result);
  set_extra(frame->f_code, extra);
  // Re-enable custom behavior
  eval_frame_callback_set(callback); // é‡ç½®å›è°ƒå‡½æ•°ï¼š
  return eval_custom_code(tstate, frame, extra->code, throw_flag); // åˆ›å»ºPyFrameObjectï¼Œå¹¶è¿è¡Œã€‚frame->f_code åŸå§‹function å­—èŠ‚ç ï¼Œextra->code æ˜¯dynamoç¼–è¯‘å®Œæˆçš„functionå­—èŠ‚ç ã€‚
} else {
  DEBUG_TRACE("create skip %s", name(frame));
  Py_DECREF(result);
  destroy_cache_entry(extra);
  set_extra(frame->f_code, SKIP_CODE);
  // Re-enable custom behavior
  eval_frame_callback_set(callback);
  return eval_frame_default(tstate, frame, throw_flag);
}
```
ä»¥ä¸Šé¢å¸¦æœ‰æ§åˆ¶æµçš„ä¿®æ”¹åçš„å­—èŠ‚ç ä¸ºä¾‹ï¼š

```
222           0 LOAD_GLOBAL              3 (__compiled_fn_0) # å¯¹åº”ä¸Šè¿°æµç¨‹ï¼Œå‰é¢ä¹Ÿä¸¾äº†ä¾‹å­è®²ï¼Œæœ‰å…´è¶£å¯ä»¥è‡ªå·±æ¨ä¸€ä¸‹ã€‚
              2 LOAD_FAST                0 (a) # compiled_fn_0
              4 LOAD_FAST                1 (b)
              6 CALL_FUNCTION            2
              8 UNPACK_SEQUENCE          2 # ç”¨äºæ‹†åˆ†ä½œä¸ºå…ƒç»„è¿”å›çš„å‡½æ•°ç»“æœ
              # æ ˆä¸Šè¿˜æœ‰TensorVariable x (live tensor) ä½œä¸º TensorVariableä¹‹åè¿˜ä¼šè¢«ç”¨åˆ°
             10 STORE_FAST               2 (x) 
             12 POP_JUMP_IF_FALSE       24
             14 LOAD_GLOBAL              4 (__resume_at_30_1) # if ä¸ºTrueçš„å­—èŠ‚ç æŒ‡ä»¤ 30_1 æ˜¯æŒ‡ä»¤çš„åç§»é‡
             16 LOAD_FAST                1 (b)
             18 LOAD_FAST                2 (x)
             20 CALL_FUNCTION            2
             22 RETURN_VALUE
        >>   24 LOAD_GLOBAL              5 (__resume_at_38_2) # if ä¸ºFalseçš„å­—èŠ‚ç æŒ‡ä»¤
             26 LOAD_FAST                1 (b)
             28 LOAD_FAST                2 (x)
             30 CALL_FUNCTION            2
             32 RETURN_VALUE
```
**åœ¨æ²¡æœ‰Dynamo çš„ä¸Šä¸‹æ–‡ä¸‹ä¸­**æ‰§è¡Œ`__compiled_fn_0`(å·²ç¼–è¯‘çš„function ä¸ä¼šå†è§¦å‘Dynamoç¼–è¯‘)ï¼Œ**è¿™ä¸ªè¿‡ç¨‹ä¸ PyTorch ä»¥ eager æ¨¡å¼æ‰§è¡Œä¸€ä¸ªå‡½æ•°ä¸€æ ·ã€‚**

**     æ‰§è¡Œå®Œç¼–è¯‘è¿‡çš„å­å›¾ **`__compiled_fn_0()`**ï¼Œç¨‹åºè¿”å›åˆ° Python è§£é‡Šå™¨ï¼Œè§£é‡Šå™¨æ‰§è¡Œé‡åˆ°çš„å­—èŠ‚ç æ˜¯ **`POP_JUMP_IF_FALSE 24`æ—¶ï¼Œ ä¼šä» Python è™šæ‹Ÿæœºæ ˆé¡¶å–å‡ºä¸€ä¸ªå…ƒç´ ï¼Œå°†å…¶è½¬åŒ–ä¸º `bool` ç±»å‹ï¼Œæ ¹æ®ç»“æœåˆ¤æ–­æ˜¯å¦è·³è½¬ã€‚æ­¤æ—¶çš„æ ˆé¡¶ä¸º Tensor `b.sum() < 0`ï¼ŒPython è™šæ‹Ÿæœºè°ƒç”¨ `THPVariable_bool_scalar()` å°†å…¶è½¬ä¸º `bool` å€¼ï¼Œæ­¤æ¬¡æ±‚å€¼ç»“æœä¸º `False`ï¼Œå› æ­¤è·³è½¬åˆ° offset 24 å¤„ å¼€å§‹ç»§ç»­æ‰§è¡Œã€‚

     Offset 24 å¤„æ˜¯æ­¤å‰ Dynamo ç”±äº graph break è€Œåˆ›å»ºçš„å‡½æ•° `__resume_at_38_2()`ã€‚**æ‰§è¡Œè¯¥ Python å‡½æ•°ä¼šè§¦å‘ Dynamo è®¾ç½®çš„ Frame Evaluation å‡½æ•° **`custom_eval_frame_shim()`ï¼Œåœ¨ [_custom_eval_frame()](https://github.com/pytorch/pytorch/blob/fe05266/torch/csrc/dynamo/eval_frame.c#L640) æ£€æŸ¥ `__resume_at_38_2()` æ˜¯å¦åœ¨ç¼“å­˜æ± ä¸­å·²ç»æœ‰ç¼–è¯‘å¥½çš„ç»“æœäº†ï¼Œå› ä¸ºæ˜¯ç¬¬ä¸€æ¬¡ç¼–è¯‘è¯¥å‡½æ•°ï¼Œæ­¤å¤„å‘ç”Ÿ cache missï¼Œæ‰€ä»¥é€šè¿‡ `call_callback()` è°ƒç”¨è®¾ç½®å¥½çš„å›è°ƒå‡½æ•° [catch_errors](https://github.com/pytorch/pytorch/blob/fe05266/torch/_dynamo/eval_frame.py#L362)ï¼Œ**é’ˆå¯¹ **`__resume_at_38_2()`** å¼€å¯ä¸€æ¬¡å…¨æ–°çš„å­å›¾ç¼–è¯‘è¿‡ç¨‹**ã€‚

 å€¼å¾—æ³¨æ„çš„æ˜¯ï¼šæ¯ä¸€æ¬¡çš„å‡½æ•°è°ƒç”¨éƒ½ä¼šæ–°å»ºä¸€ä¸ªPyFrameObject

### Guards
æˆ‘ä»¬å†æ¥çœ‹ä¸‹ä¸€ä¸ªå¾ˆé‡è¦çš„æ¨¡å—ï¼š`Guards`, å®ƒæœ‰å¤šé‡è¦å‘¢ï½Dynamo æ¯æ¬¡æŠ“åˆ°Fx Graphä¹‹åï¼Œäº¤ç»™backend compiler ç¼–è¯‘ï¼Œcompiler ä¼˜åŒ–ä¸­æœ‰ä¸ªå¾ˆé‡è¦çš„ç‚¹ï¼šcache ç¼–è¯‘ä¹‹åçš„å›¾ï¼ˆåšäº†ä¸€äº›ä¼˜åŒ–ï¼‰ï¼Œè¿™æ ·å†ä¸‹ä¸€ä¸ªstepæ‰§è¡Œæ—¶ï¼Œå¦‚æœå›¾ä¿¡æ¯æ²¡å˜ï¼Œç›´æ¥ä»cache poolä¸­æ‰¾åˆ°è¯¥å›¾æ‰§è¡Œã€‚ä¸€æ¬¡ç¼–è¯‘å¤šæ¬¡æ‰§è¡Œï½ã€‚æ‰€ä»¥éœ€è¦è§£å†³ â€œåœ¨ä¸‹ä¸€ä¸ªstepç¡®å®šæ˜¯å¦è¦é‡æ–°trace æ–°çš„å›¾â€è¿™ä»¶äº‹æƒ…ã€‚`Guardså°±æ˜¯ç”¨æ¥åšè¿™ä¸ªäº‹æƒ…ã€‚å¾ˆé‡è¦å§å“ˆå“ˆå“ˆ`ã€‚

![](./dynamo.png "")
Guards æ˜¯å¦‚ä½•æ¥åšçš„å‘¢ï¼Ÿ

å­—èŠ‚ç ç¿»è¯‘è¿‡ç¨‹æˆ‘ä»¬æåˆ°äº†ï¼šåˆ›å»º`VariableTrack`, å®é™…ä¸Šæ¯æ¬¡åˆ›å»º`VariableTrack`éƒ½ä¼šåˆ›å»ºä¸€ä¸ª`Guard`ï¼Œé€æ­¥å°†**ä»è¾“å…¥æ”¶é›†çš„ä¿¡æ¯ä¼ æ’­åˆ°è¾“å‡ºèŠ‚ç‚¹ã€‚ä»¥ç¬¬ä¸€ä¸ªexampleä¸ºä¾‹ï¼š**3 ä¸ª Guardåˆ†åˆ«æ˜¯å¼ é‡ x å’Œ yã€åº“torchã€‚

![](./dynamo_convert_frame.png "")
æ¯ä¸ªFunctionéƒ½è¦æœ‰ä¸ªcheckå‡½æ•°ï¼Œç”¨äºæ£€æŸ¥è¾“å…¥ä¿¡æ¯æ˜¯å¦ä¿®æ”¹ã€‚`TENSOR_MATCH` :åœ¨é»˜è®¤æƒ…å†µä¸‹ï¼Œä¸»è¦è´Ÿè´£æ£€æŸ¥è¾“å…¥çš„å¼ é‡ deviceã€shapeã€stride ç­‰å±æ€§æ˜¯å¦æ”¹å˜ã€‚ `FUNCTION_MATCH`å¯èƒ½æ˜¯ç”¨æ¥check ç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°çš„ï¼Ÿï¼ˆè¿™é‡Œæˆ‘è¿˜æ²¡æœ‰å…‘å‡ºæ¥å…·ä½“çš„æ¡ˆä¾‹å“ˆå“ˆå“ˆï¼Œä¹‹åé‡åˆ°äº†åº”è¯¥å°±æ˜ç™½äº†ã€‚ï¼‰

```python
 # æˆ‘ä»¬éœ€è¦å»è‡ªå®šä¹‰check function
check_fn = CheckFunctionManager(
       output,
       locals,
       globals,
       hooks.guard_fail_fn if hooks else None,
 )
 guarded_code = GuardedCode(out_code, check_fn.check_fn) # ä¿å­˜äº†ç¼–è¯‘å¥½çš„å­å›¾å’Œcheck_fn
```
TorchDynamo ä¼šä¸ºè¢«ç¼–è¯‘çš„å‡½æ•°åˆ›å»º `Guard`ã€‚

```python
@dataclasses.dataclass
class GuardedCode:
    code: types.CodeType #python code object (ç¼–è¯‘ä¹‹åçš„å­å›¾)
    check_fn: GuardFn 
    #check_fn æä¾›äº†ä»ç®€å•æ•°æ®ç±»è½¬æ¢ä¸ºå®é™…ç”Ÿæˆè¦è°ƒç”¨çš„æœ‰æ•ˆPythonä»£ç çš„ä¸»è¦åŠŸèƒ½ï¼Œä»¥ä¾¿äº†è§£è°ƒç”¨ä¹‹é—´çš„æƒ…å†µæ˜¯å¦å‘ç”Ÿå˜åŒ–ï¼Œä»¥åŠæˆ‘ä»¬æ˜¯å¦å¯ä»¥å®‰å…¨åœ°ä»ä»£ç ç¼“å­˜ä¸­è¯»å–ã€‚
```
`GuardedCode` ä¼šè¢«å­˜åˆ°cached_entryé‡Œã€‚ç”¨æˆ·å‡½æ•°çš„ `frame->f_code` ä¸­å†™å…¥ä¸€æ¡ [CacheEntry](https://github.com/pytorch/pytorch/blob/fe05266/torch/csrc/dynamo/eval_frame.c#L415)ï¼Œè®°å½•äº† `check_fn` å’Œç¼–è¯‘å¥½çš„ `code`ã€‚

```c
static CacheEntry *create_cache_entry(CacheEntry *next,
                                      PyObject *guarded_code) {
  CacheEntry *e = (CacheEntry *)malloc(sizeof(CacheEntry));
  DEBUG_NULL_CHECK(e);
  e->check_fn = PyObject_GetAttrString(guarded_code, "check_fn");
  NULL_CHECK(e->check_fn);
  e->code = (PyCodeObject *)PyObject_GetAttrString(guarded_code, "code");
  NULL_CHECK(e->code);
  e->next = next;
  return e;
}
```
